{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Unified Financial Statement Data Retriever v2\n",
                "This notebook provides three methods for extracting data from financial statements: `pypdf`, `ocr`, and `html`. \n",
                "It follows with LLM-based parsing (Gemini) and Excel conversion."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import io\n",
                "import json\n",
                "import re\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "from pypdf import PdfReader\n",
                "import pytesseract\n",
                "import fitz  # PyMuPDF, used as backend for OCR\n",
                "from PIL import Image\n",
                "from bs4 import BeautifulSoup\n",
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "config_header",
            "metadata": {},
            "source": [
                "## 1. Configuration and User Input"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "config",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Configured for: CALM\n",
                        "Periods: ['2021', '2022', '2023']\n",
                        "Method: HTML\n",
                        "Source Directory: financial_statement\n",
                        "Page range: 20-70\n"
                    ]
                }
            ],
            "source": [
                "# --- User Input ---\n",
                "company_folder_name = input(\"Enter the company folder name (e.g., PVIAM): \").strip()\n",
                "periods_input = input(\"Enter periods to process (list e.g., 2021, 2022 OR range e.g., 2020-2024): \").strip()\n",
                "\n",
                "# Parse periods\n",
                "periods_to_process = []\n",
                "if '-' in periods_input:\n",
                "    try:\n",
                "        start_y, end_y = map(int, re.findall(r'\\d{4}', periods_input))\n",
                "        periods_to_process = [str(y) for y in range(start_y, end_y + 1)]\n",
                "    except ValueError:\n",
                "        print(\"Could not parse range. Trying as list.\")\n",
                "        periods_to_process = [p.strip() for p in periods_input.split(',') if p.strip()]\n",
                "else:\n",
                "    periods_to_process = [p.strip() for p in periods_input.split(',') if p.strip()]\n",
                "\n",
                "extraction_method = \"\"\n",
                "while extraction_method not in [\"ocr\", \"pypdf\", \"html\"]:\n",
                "    extraction_method = input(\"Choose extraction method (ocr / pypdf / html): \").strip().lower()\n",
                "\n",
                "page_range_input = input(\"Enter page range (e.g., 50-90, leave blank for all): \").strip()\n",
                "start_page, end_page = None, None\n",
                "if page_range_input and '-' in page_range_input:\n",
                "    try:\n",
                "        s, e = page_range_input.split('-')\n",
                "        start_page, end_page = int(s), int(e)\n",
                "    except ValueError:\n",
                "        print(\"Invalid range format. Processing all pages.\")\n",
                "\n",
                "# --- Paths ---\n",
                "base_workspace = Path(r\"D:\\Visual Studio Projects\\Financial Statement Data Retriever\")\n",
                "company_base_path = base_workspace / company_folder_name \n",
                "\n",
                "# Try plural first, then singular\n",
                "base_pdf_dir = company_base_path / \"financial_statements\"\n",
                "if not base_pdf_dir.exists():\n",
                "    base_pdf_dir = company_base_path / \"financial_statement\"\n",
                "\n",
                "text_dir = company_base_path / \"text_statements\"\n",
                "json_dir = company_base_path / \"json_statements\"\n",
                "excel_dir = company_base_path / \"excel_statements\"\n",
                "\n",
                "for d in [text_dir, json_dir, excel_dir]:\n",
                "    d.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"\\nConfigured for: {company_folder_name}\")\n",
                "print(f\"Periods: {periods_to_process}\")\n",
                "print(f\"Method: {extraction_method.upper()}\")\n",
                "if base_pdf_dir.exists():\n",
                "    print(f\"Source Directory: {base_pdf_dir.name}\")\n",
                "else:\n",
                "    print(f\"Warning: Source directory not found in {company_base_path}\")\n",
                "if start_page:\n",
                "    print(f\"Page range: {start_page}-{end_page}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "extraction_header",
            "metadata": {},
            "source": [
                "## 2. Text Extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "extraction_logic",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Starting HTML Extraction ---\n",
                        "Processing 2024 via HTML...\n",
                        "Saved to: 2024_html.txt\n",
                        "Processing 2025 via HTML...\n",
                        "Saved to: 2025_html.txt\n"
                    ]
                }
            ],
            "source": [
                "def extract_from_html(file_path, start_page=None, end_page=None):\n",
                "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
                "        soup = BeautifulSoup(f, \"html.parser\")\n",
                "        \n",
                "        # Remove script and style elements\n",
                "        for script_or_style in soup([\"script\", \"style\"]):\n",
                "            script_or_style.decompose()\n",
                "            \n",
                "        # Identify page break markers\n",
                "        # SEC filings often use styles like page-break-after:always or <hr>\n",
                "        # We'll wrap segments between breaks into virtual pages\n",
                "        pages = []\n",
                "        current_page_elements = []\n",
                "        \n",
                "        # Traverse the body to find breaks\n",
                "        body = soup.find(\"body\") or soup\n",
                "        for element in body.descendants:\n",
                "            if element.name in [\"div\", \"p\", \"hr\", \"table\"]:\n",
                "                style = element.get(\"style\", \"\")\n",
                "                if \"page-break\" in style or element.name == \"hr\":\n",
                "                    if current_page_elements:\n",
                "                        pages.append(\" \".join(current_page_elements))\n",
                "                        current_page_elements = []\n",
                "                \n",
                "                # Only add direct strings to avoid redundant nested text\n",
                "                if hasattr(element, \"strings\"):\n",
                "                    text = element.get_text(\" \", strip=True)\n",
                "                    if text and (not current_page_elements or text not in current_page_elements[-1]):\n",
                "                        current_page_elements.append(text)\n",
                "        \n",
                "        if current_page_elements:\n",
                "            pages.append(\" \".join(current_page_elements))\n",
                "            \n",
                "        # If no breaks found, treat whole as one page\n",
                "        if not pages:\n",
                "            pages = [soup.get_text(\" \", strip=True)]\n",
                "            \n",
                "        total_pages = len(pages)\n",
                "        pages_to_extract = range(total_pages)\n",
                "        if start_page and end_page:\n",
                "            pages_to_extract = range(max(0, start_page - 1), min(total_pages, end_page))\n",
                "            \n",
                "        text_content = []\n",
                "        for i in pages_to_extract:\n",
                "            text_content.append(f\"--- PAGE {i+1} ---\\n{pages[i]}\\n\")\n",
                "            \n",
                "        return \"\\n\".join(text_content)\n",
                "\n",
                "def extract_text_from_pdf(pdf_path, method, start_page=None, end_page=None):\n",
                "    text_content = []\n",
                "    \n",
                "    if method == \"pypdf\":\n",
                "        reader = PdfReader(pdf_path)\n",
                "        total_pages = len(reader.pages)\n",
                "        pages_to_extract = range(total_pages)\n",
                "        if start_page and end_page:\n",
                "            pages_to_extract = range(max(0, start_page - 1), min(total_pages, end_page))\n",
                "            \n",
                "        for i in pages_to_extract:\n",
                "            page = reader.pages[i]\n",
                "            text = page.extract_text()\n",
                "            text_content.append(f\"--- PAGE {i+1} ---\\n{text}\\n\")\n",
                "            \n",
                "    elif method == \"ocr\":\n",
                "        doc = fitz.open(pdf_path)\n",
                "        total_pages = len(doc)\n",
                "        pages_to_extract = range(total_pages)\n",
                "        if start_page and end_page:\n",
                "            pages_to_extract = range(max(0, start_page - 1), min(total_pages, end_page))\n",
                "\n",
                "        for i in pages_to_extract:\n",
                "            page = doc.load_page(i)\n",
                "            pix = page.get_pixmap(dpi=300)\n",
                "            img_bytes = pix.tobytes(\"png\")\n",
                "            img = Image.open(io.BytesIO(img_bytes))\n",
                "            text = pytesseract.image_to_string(img, lang=\"vie+eng\", config=\"--psm 3\")\n",
                "            text_content.append(f\"--- PAGE {i+1} ---\\n{text}\\n\")\n",
                "        doc.close()\n",
                "        \n",
                "    return \"\\n\".join(text_content)\n",
                "\n",
                "print(f\"--- Starting {extraction_method.upper()} Extraction ---\")\n",
                "for period in periods_to_process:\n",
                "    if extraction_method == \"html\":\n",
                "        # Check for .html or .htm\n",
                "        file_path = base_pdf_dir / f\"{period}.html\"\n",
                "        if not file_path.exists():\n",
                "            file_path = base_pdf_dir / f\"{period}.htm\"\n",
                "    else:\n",
                "        file_path = base_pdf_dir / f\"{period}.pdf\"\n",
                "        \n",
                "    out_txt = text_dir / f\"{period}_{extraction_method}.txt\"\n",
                "    \n",
                "    if not file_path.exists():\n",
                "        print(f\"Skipping {period}: File not found at {file_path}\")\n",
                "        continue\n",
                "        \n",
                "    print(f\"Processing {period} via {extraction_method.upper()}...\")\n",
                "    try:\n",
                "        if extraction_method == \"html\":\n",
                "            extracted_text = extract_from_html(file_path, start_page, end_page)\n",
                "        else:\n",
                "            extracted_text = extract_text_from_pdf(file_path, extraction_method, start_page, end_page)\n",
                "            \n",
                "        with out_txt.open(\"w\", encoding=\"utf-8\") as f:\n",
                "            f.write(extracted_text)\n",
                "        print(f\"Saved to: {out_txt.name}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error processing {period}: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "llm_header",
            "metadata": {},
            "source": [
                "## 3. LLM Data Extraction (Gemini)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "llm_logic",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Starting LLM Extraction ---\n",
                        "Invoking Gemini for 2024...\n",
                        "Saved raw JSON to: 2024_raw.json\n",
                        "Invoking Gemini for 2025...\n",
                        "Saved raw JSON to: 2025_raw.json\n"
                    ]
                }
            ],
            "source": [
                "# Set API Key\n",
                "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyANqiR6J33QWw1O37AqDNEjfi17whkaJ1g\" \n",
                "\n",
                "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.05)\n",
                "\n",
                "prompt_template = ChatPromptTemplate.from_messages(\n",
                "    [\n",
                "        (\"system\", \"You are an expert financial analyst. Your task is to extract various line items and their values from the provided text. \"\n",
                "                   \"Output the extracted data as a JSON array of objects, where each object has 'item_number' (if there is item number, or else leave blank),'statement_type', 'item', 'year', and 'value'. \"\n",
                "                   \"Ensure values are numeric (remove commas, currency symbols, etc.) or leave empty if not found.\"\n",
                "                   \"Ensure that the line items, as well as the name of the statements are the same as the language being used in the text.\"\n",
                "                   \"Sometimes there can be grammatial error and line item numering error, make sure to fix it as well, don't be too rigid\"\n",
                "                   \"ONLY take the current year from this statement, not the last years.\"\n",
                "                   \"Make sure that the line items are in proper form, that is no FULL CAPITALIZTATION, and only First Letter Capitalization\"\n",
                "                   \"The name of the statements must be consistent and indifferent as given from the prompt.\"),\n",
                "        (\"human\", \"Extract information from the 3 financial statements including: Income Statement, Balance Sheet, and Statement of Cash Flows, use the aforementioned categorey as the names for statement_type, do not put different names' :\\n\\n{text}\")\n",
                "    ]\n",
                ")\n",
                "\n",
                "chain = prompt_template | llm | StrOutputParser()\n",
                "\n",
                "print(\"--- Starting LLM Extraction ---\")\n",
                "for period in periods_to_process:\n",
                "    txt_file = text_dir / f\"{period}_{extraction_method}.txt\"\n",
                "    out_json = json_dir / f\"{period}_raw.json\"\n",
                "    \n",
                "    if not txt_file.exists():\n",
                "        print(f\"Skipping {period}: Text file not found.\")\n",
                "        continue\n",
                "        \n",
                "    print(f\"Invoking Gemini for {period}...\")\n",
                "    try:\n",
                "        with txt_file.open(\"r\", encoding=\"utf-8\") as f:\n",
                "            content = f.read()\n",
                "            \n",
                "        response = chain.invoke({\"text\": content})\n",
                "        with out_json.open(\"w\", encoding=\"utf-8\") as f:\n",
                "            f.write(response)\n",
                "        print(f\"Saved raw JSON to: {out_json.name}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error with LLM for {period}: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "excel_header",
            "metadata": {},
            "source": [
                "## 4. Excel Conversion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "excel_logic",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Starting Excel Conversion ---\n",
                        "Converted 2021 to Excel: 2021_statements.xlsx (95 items)\n",
                        "Converted 2022 to Excel: 2022_statements.xlsx (101 items)\n",
                        "Converted 2023 to Excel: 2023_statements.xlsx (96 items)\n",
                        "\n",
                        "Done!\n"
                    ]
                }
            ],
            "source": [
                "print(\"--- Starting Excel Conversion ---\")\n",
                "for period in periods_to_process:\n",
                "    json_file = json_dir / f\"{period}_raw.json\"\n",
                "    out_excel = excel_dir / f\"{period}_statements.xlsx\"\n",
                "    \n",
                "    if not json_file.exists():\n",
                "        continue\n",
                "        \n",
                "    try:\n",
                "        with json_file.open(\"r\", encoding=\"utf-8\") as f:\n",
                "            raw_data = f.read()\n",
                "        \n",
                "        # Clean JSON markdown blocks\n",
                "        clean_json = re.sub(r'^```json\\s*|\\s*```$', '', raw_data.strip(), flags=re.MULTILINE)\n",
                "        data = json.loads(clean_json)\n",
                "        \n",
                "        # Handle nested data if present\n",
                "        if isinstance(data, dict):\n",
                "            data = data.get(\"financial_statements\", data.get(\"data\", data))\n",
                "            \n",
                "        if isinstance(data, list):\n",
                "            df = pd.DataFrame(data)\n",
                "            if 'value' in df.columns:\n",
                "                df['value'] = pd.to_numeric(df['value'].astype(str).str.replace(',', '').str.strip(), errors='coerce')\n",
                "            \n",
                "            df.to_excel(out_excel, index=False)\n",
                "            print(f\"Converted {period} to Excel: {out_excel.name} ({len(df)} items)\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error converting {period}: {e}\")\n",
                "\n",
                "print(\"\\nDone!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "generic",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
