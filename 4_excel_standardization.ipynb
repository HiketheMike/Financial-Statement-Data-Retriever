{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa836bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company folder set to: PVI Insurance\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# --- User Input for Company Folder ---\n",
    "# Enter the name of the company folder (e.g., \"PVIAM\", \"PVI Insurance\")\n",
    "company_folder_name = input(\"Enter the company folder name (e.g., PVIAM): \").strip()\n",
    "\n",
    "if not company_folder_name:\n",
    "    print(\"No company folder name entered. Please run this cell again and provide a name.\")\n",
    "    # You might want to exit or raise an error here if a company name is mandatory\n",
    "else:\n",
    "    print(f\"Company folder set to: {company_folder_name}\")\n",
    "\n",
    "# Construct the base path for the company's data\n",
    "# This assumes your company folders are directly under 'Financial Statement Data Retriever'\n",
    "company_base_path = Path(r\"D:\\Visual Studio Projects\\Financial Statement Data Retriever\") / company_folder_name\n",
    "\n",
    "# The periods_input and periods_to_process are not needed for this notebook\n",
    "# as it processes files based on statement type names from the final_statements folder.\n",
    "# Keeping this section commented out for clarity if you ever need period context.\n",
    "# --- User Input for Periods (Optional, but good for context if needed later) ---\n",
    "# periods_input = input(\"Enter periods to process (e.g., 2021, 2022, 2023) or leave blank: \")\n",
    "# periods_to_process = [p.strip() for p in periods_input.split(',') if p.strip()]\n",
    "# if not periods_to_process:\n",
    "#     print(\"No periods entered. Proceeding without period-specific filtering.\")\n",
    "# else:\n",
    "#     print(f\"Periods set for processing (for context): {periods_to_process}\")\n",
    "# periods_to_process = [\"2021\", \"2022\", \"2023\", \"2024\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3031a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Financial Statement Item Standardization ---\n",
      "\n",
      "Processing file for standardization: Báo Cáo Kết Quả Hoạt Động Kinh Doanh.xlsx\n",
      "  Found 127 unique items. Sending to Gemini for standardization...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set your Google API Key here.\n",
    "# It's recommended to set this as an environment variable (e.g., GOOGLE_API_KEY)\n",
    "# **CRITICAL: Replace the placeholder below with your actual, valid Google API Key**\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyD1f3CDdw71J98b4LEFFM6IUY893qfnqdg\" \n",
    "\n",
    "# Define input and output directories using the dynamically set company_base_path\n",
    "input_dir = company_base_path / \"final_statements\"\n",
    "output_dir = company_base_path / \"final_statements_standardized\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Initialize Gemini 2.5 Flash LLM ---\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5)\n",
    "\n",
    "# --- Define Prompt for Financial Item Standardization ---\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert financial analyst specializing in Vietnamese financial statements. \"\n",
    "                   \"Your task is to standardize financial statement line items. \"\n",
    "                   \"You will be given a list of items, which may contain variations due to OCR errors, slightly different phrasing, or garbled numbering. \"\n",
    "                   \"For each group of semantically similar items, identify them and propose a single, concise, and commonly accepted standardized Vietnamese name. \"\n",
    "                   \"The standardized name should be in proper case (first letter of each word capitalized). \"\n",
    "                   \"Prioritize standardized names that include a line item number (e.g., 'I. Tiền và các khoản tương đương tiền') if available among the original items. \"\n",
    "                   \"If a 'TOTAL' or 'TỔNG CỘNG' item exists, ensure its standardized name clearly reflects it as a total. \"\n",
    "                   \"Make sure that the item that has the same name, isn't containing different values from each other. Since there could be sub-items that have the same name but bear different values for their parent item.\"\n",
    "                   \"Output the mapping as a JSON array of objects. Each object in the array should represent a standardized item and contain two keys: \"\n",
    "                   \"'standardized_item' (the proposed standardized name) and 'original_items' (a list of all original items that map to this standardized name). \"\n",
    "                   \"The order of objects in the JSON array MUST represent the logical order of items in a financial statement (e.g., assets before liabilities, short-term before long-term, and within sections, by line item number if present). \"\n",
    "                   \"Ensure all original items from the input list are present in your output mapping under their respective standardized items.\"),\n",
    "        (\"human\", \"Standardize the following financial statement items:\\n\\n{items_list_json}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Create LangChain Chain ---\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt_template | llm | output_parser\n",
    "\n",
    "print(\"--- Starting Financial Statement Item Standardization ---\")\n",
    "\n",
    "# Process each Excel file in the input directory\n",
    "for file_path in input_dir.glob(\"*.xlsx\"):\n",
    "    print(f\"\\nProcessing file for standardization: {file_path.name}\")\n",
    "    try:\n",
    "        # Read the reformatted Excel file (item is already the index)\n",
    "        df_wide = pd.read_excel(file_path, index_col=0)\n",
    "\n",
    "        if df_wide.empty:\n",
    "            print(f\"  Warning: {file_path.name} is empty. Skipping standardization.\")\n",
    "            continue\n",
    "\n",
    "        # Get the list of unique items (index values) to send to the LLM\n",
    "        items_to_standardize = df_wide.index.astype(str).unique().tolist()\n",
    "\n",
    "        if not items_to_standardize:\n",
    "            print(f\"  No items found in {file_path.name} to standardize. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  Found {len(items_to_standardize)} unique items. Sending to Gemini for standardization...\")\n",
    "        \n",
    "        # Convert the list of items to a JSON string for the LLM prompt\n",
    "        items_list_json = json.dumps(items_to_standardize, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Invoke LLM for standardization mapping\n",
    "        llm_response = chain.invoke({\"items_list_json\": items_list_json})\n",
    "        \n",
    "        print(f\"  Received standardization mapping from Gemini for {file_path.name}.\")\n",
    "\n",
    "        # Clean and parse the LLM's JSON response\n",
    "        cleaned_json_string = llm_response.strip()\n",
    "        if cleaned_json_string.startswith(\"```json\"):\n",
    "            cleaned_json_string = cleaned_json_string[len(\"```json\"):].strip()\n",
    "        if cleaned_json_string.endswith(\"```\"):\n",
    "            cleaned_json_string = cleaned_json_string[:-len(\"```\")].strip()\n",
    "\n",
    "        standardization_groups = json.loads(cleaned_json_string)\n",
    "        \n",
    "        # Create a dictionary for mapping: {original_item: standardized_item}\n",
    "        item_mapping = {}\n",
    "        # Create an ordered list of standardized items\n",
    "        ordered_standardized_items = []\n",
    "\n",
    "        for group in standardization_groups:\n",
    "            standardized_name = group['standardized_item']\n",
    "            ordered_standardized_items.append(standardized_name)\n",
    "            for original_item in group['original_items']:\n",
    "                item_mapping[original_item] = standardized_name\n",
    "\n",
    "        # Apply the mapping to the DataFrame's index\n",
    "        df_temp = df_wide.rename(index=item_mapping)\n",
    "\n",
    "        # Handle potential duplicates after standardization (e.g., if \"Cash\" and \"Cash and equivalents\" both map to \"Cash\")\n",
    "        # We sum the values for items that now share the same standardized name.\n",
    "        df_aggregated = df_temp.groupby(df_temp.index).sum()\n",
    "\n",
    "        # Reindex the aggregated DataFrame to enforce Gemini's desired order\n",
    "        # Any standardized items from Gemini's list not present in df_aggregated will be added as NaN rows.\n",
    "        # Any items in df_aggregated not in Gemini's ordered list will be dropped (this shouldn't happen if Gemini maps all).\n",
    "        df_standardized = df_aggregated.reindex(ordered_standardized_items)\n",
    "\n",
    "        # Define the output path in the final_statements_standardized directory\n",
    "        output_file_path = output_dir / file_path.name\n",
    "        \n",
    "        # Save the standardized DataFrame to a new Excel file\n",
    "        df_standardized.to_excel(output_file_path)\n",
    "        print(f\"  Successfully standardized and saved '{file_path.name}' to: {output_file_path}\")\n",
    "        print(f\"  Final standardized DataFrame shape: {df_standardized.shape}\")\n",
    "        print(f\"  Final standardized DataFrame head:\\n{df_standardized.head()}\")\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"  ERROR: JSON decoding failed for LLM response for {file_path.name}: {e}\")\n",
    "        print(f\"  LLM Response (raw):\\n{llm_response}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR processing {file_path.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n--- Financial Statement Item Standardization Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
